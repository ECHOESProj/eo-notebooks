{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from xcube_sh.sentinelhub import SentinelHub\n",
    "    from sentinelhub import BBox, WmsRequest, DataSource, SHConfig\n",
    "    from xcube_sh.observers import Observers\n",
    "    from xcube_sh.config import CubeConfig\n",
    "    from xcube_sh.cube import open_cube\n",
    "    raise ModuleNotFoundError\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "from sklearn.decomposition import PCA, SparsePCA, MiniBatchSparsePCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "# from edc import setup_environment_variables\n",
    "from os import environ\n",
    "import xarray as xr\n",
    "from pyproj import Transformer\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "import IPython.display\n",
    "import shapely.geometry\n",
    "import rioxarray\n",
    "from zarr.errors import GroupNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bbox = (-3.93, 52.56, -3.9, 52.58)\n",
    "# bbox = (32,  31.1, 32.1, 31.2) # Middle East\n",
    "bbox = (-3.93, 52.56, -3.9, 52.58) \n",
    "IPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -r data/*.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# time_range = ['2021-06-01', '2021-06-30']\n",
    "# spatial_res = 4*0.00018   # = 10.038 meters in degree>\n",
    "# cube_config = CubeConfig(dataset_name='S2L2A',\n",
    "#                          band_names=['B02', 'B03', 'B04', 'CLM'],\n",
    "#                          #tile_size=[512, 512],\n",
    "#                          bbox=bbox,\n",
    "#                          spatial_res=spatial_res,\n",
    "#                          time_range=time_range,\n",
    "#                          time_period='1D')\n",
    "\n",
    "# request_collector = Observers.request_collector()\n",
    "# try:\n",
    "#     cube = xr.open_zarr('data/cube1.zarr')\n",
    "# except GroupNotFoundError:\n",
    "#     cube = open_cube(cube_config, observer=request_collector)\n",
    "#     cube.to_zarr('data/cube1.zarr')\n",
    "# # cube = cube.rename(lon='x', lat='y')\n",
    "\n",
    "# filt = np.logical_not(np.isnan(cube['B02']).all(dim=('lon', 'lat')))\n",
    "# cube = cube.where(filt, drop=True)\n",
    "# # cube\n",
    "# cube = cube.where(np.logical_not(np.isnan(cube)), cube.mean(dim=('time')))\n",
    "\n",
    "# cube = cube.mean(dim='time')\n",
    "\n",
    "# cube['B02'] = cube['B02'].where(np.isnan(cube['B02']).mean(dim=('lon', 'lat')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "band_names = [f'B0{i}' for i in range(2, 10) if i != 9] + ['B8A', 'CLM', 'SCL']\n",
    "print(band_names)\n",
    "\n",
    "spatial_res = 1*0.00018   # = 10.038 meters in degre\n",
    "\n",
    "def get_cloud_free(time_range):\n",
    "    try:\n",
    "        spatial_res = 0.00018   # = 10.038 meters in degree>\n",
    "        cube_config = CubeConfig(dataset_name='S2L2A',\n",
    "                                 band_names=band_names,\n",
    "                                 #tile_size=[512, 512],\n",
    "                                 bbox=bbox,\n",
    "                                 spatial_res=spatial_res,\n",
    "                                 time_range=time_range,\n",
    "                                 time_period='1D')\n",
    "\n",
    "        request_collector = Observers.request_collector()\n",
    "        fname = F'data/{time_range[0]}_{time_range[1]}.zarr'\n",
    "        cube = xr.open_zarr(fname)\n",
    "    except GroupNotFoundError:\n",
    "        cube = open_cube(cube_config, observer=request_collector)\n",
    "        cube.to_zarr(fname)\n",
    "    # cube = cube.rename(lon='x', lat='y')\n",
    "\n",
    "    filt = np.logical_not(np.isnan(cube['B02']).all(dim=('lon', 'lat')))\n",
    "    cube = cube.where(filt, drop=True)\n",
    "    cube = cube.where(np.logical_not(cube.CLM))\n",
    "    # cube\n",
    "    cube = cube.mean(dim='time')\n",
    "#     cube = cube.where(np.logical_not(np.isnan(cube)), 0)\n",
    "    return cube\n",
    "# cube = cube.where(np.logical_not(np.isnan(cube)), cube.mean(dim=('time')))\n",
    "band_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d1, d2 = '2019-09-20', '2020-09-21'\n",
    "time_range = [d1, d1]\n",
    "ds_a = get_cloud_free(time_range)\n",
    "import time\n",
    "time_range = [d2, d2]\n",
    "ds_b = get_cloud_free(time_range)\n",
    "ds = xr.concat([ds_a, ds_b], dim='time')\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# def rescale(X):\n",
    "#     X = X.where(np.isfinite(X))\n",
    "#     Xmax = X.max()*0.8\n",
    "#     Xmin = X.min()*1.2\n",
    "#     return ((X - Xmin) * (1/(Xmax - Xmin) * 1))#.astype('uint8')\n",
    "\n",
    "def rescale(X):\n",
    "    return 6*X\n",
    "\n",
    "# size = 128\n",
    "# ds = ds.isel(lat=slice(0, 64), lon=slice(0, size))\n",
    "# ds_a = ds_a.isel(lat=slice(0, 64), lon=slice(0, size))\n",
    "# ds_b = ds_b.isel(lat=slice(0, 64), lon=slice(0, size))\n",
    "\n",
    "for t, ds0 in ds.groupby('time'):\n",
    "    r, g, b = 'B04', 'B03', 'B02'\n",
    "    cube_rgb = xr.concat([rescale(ds0[r]), rescale(ds0[g]), rescale(ds0[b])], 'band')\n",
    "    cube_rgb.rio.to_raster(f\"plots/{t}.tif\")\n",
    "    cube_rgb.plot.imshow(rgb='band', figsize=(7, 7))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v = 'B03'\n",
    "plt.hist2d(ds_a[v].values.ravel(), ds_b[v].values.ravel(), bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_a['SCL'].plot.imshow()\n",
    "plt.show()\n",
    "ds_b['SCL'].plot.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# time_range = ['2020-07-17', '2020-07-17']\n",
    "# ds_202106 = get_cloud_free(time_range)\n",
    "\n",
    "# time_range = ['2020-11-14', '2020-11-14']\n",
    "# ds_202006 = get_cloud_free(time_range)\n",
    "ds_a = ds_a.fillna(0)\n",
    "ds_b = ds_b.fillna(0)\n",
    "\n",
    "components_list = []\n",
    "for v in ds_a.variables:\n",
    "    print(v)\n",
    "    if not v.startswith('B'):\n",
    "        continue\n",
    "    ds = xr.concat([ds_a, ds_b], dim='time')\n",
    "    da_z= ds[v].stack(z=('lon', 'lat'))\n",
    "    pca = SparsePCA(alpha=.1, tol=1e-4, max_iter=20, method='lars', n_jobs=-1)\n",
    "#     pca = PCA()\n",
    "    weights = pca.fit_transform(da_z.isel(time=[0, 1]).T)\n",
    "    da_z = xr.DataArray(weights, dims=da_z.dims, name=v)\n",
    "    components_list.append(da_z)\n",
    "components = np.array(components_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in range(8):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    for i in range(2):\n",
    "        vmax = np.quantile(components[c,:,i], 0.99)\n",
    "        print(vmax)\n",
    "        axs[i].imshow(components[c,:,i].reshape(len(ds.lon), len(ds.lat)).T, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for c in range(8):\n",
    "#     plt.hist2d(components[c,:,0], components[c,:,1],bins=1000)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_1d(time):\n",
    "    dd = ds.isel(time=time)\n",
    "    return xr.concat([dd[v] for v in dd], 'band').stack(z=['lon', 'lat']).transpose('z', 'band')\n",
    "\n",
    "a = get_1d(0)\n",
    "b = get_1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, AffinityPropagation, DBSCAN, MeanShift\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def weights_of_diff_pca(corr_diff):\n",
    "#     pca = PCA(n_components=5)\n",
    "#     pca_comp_weights = pca.fit_transform(components[:, :, corr_diff].T)\n",
    "    pca_comp_weights = components[:, :, corr_diff].T\n",
    "    scaler = StandardScaler()\n",
    "    pca_comp_weights = scaler.fit_transform(pca_comp_weights)\n",
    "    return pca, pca_comp_weights # / np.sqrt((pca_comp_weights**2).sum(axis=0))\n",
    "\n",
    "pca0, pca_comp_weights0 = weights_of_diff_pca(0)\n",
    "pca1, pca_comp_weights1 = weights_of_diff_pca(1)\n",
    "\n",
    "da_weights0 = xr.DataArray(pca_comp_weights0, dims=('z', 'band'))\n",
    "da_weights1 = xr.DataArray(pca_comp_weights1, dims=('z', 'band'))\n",
    "\n",
    "ds_diff = xr.Dataset(data_vars=dict(weights0=da_weights0, weights1=da_weights1))\n",
    "\n",
    "def norm(x):\n",
    "    return x / np.sqrt((x**2).sum())  \n",
    "\n",
    "weights1_pwr = np.sqrt((ds_diff.weights1**2).sum(dim='band'))\n",
    "# ds_diff = ds_diff.where( weights1_pwr > np.quantile(weights1_pwr, 0.95))\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# for v in ['weights0', 'weights1']:\n",
    "#     ds_diff[v] = (ds_diff[v].dims, scaler.fit_transform(ds_diff[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.concatenate([pca_comp_weights0.ravel(), pca_comp_weights1.ravel()]), bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt((pca_comp_weights0**2).sum(axis=1)), bins=200, alpha=1)\n",
    "# plt.hist(np.sqrt((pca_comp_weights1**2).sum(axis=1)), bins=200, alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.abs(ds_diff['weights0'].mean(dim='band')).plot.hist(bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for (b0, d0), (b1, d1) in zip(*(ds_diff['weights1'].groupby('band'), ds_diff['weights1'].groupby('band'))):\n",
    "    d0.plot.hist(bins=1000, alpha=1)\n",
    "#     d1.plot.hist(bins=1000, alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. If linkage is “ward”, only “euclidean” is accepted. If “precomputed”, a distance matrix (instead of a similarity matrix) is needed as input for the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import FastICA\n",
    "X = ds_diff['weights1']\n",
    "transformer = FastICA(n_components=1, whiten=True, random_state=0)\n",
    "X_transformed = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = np.sqrt((ds_diff['weights1']**2).sum(axis=1)).values.reshape(len(ds.lon), len(ds.lat)).T\n",
    "plt.imshow(a**2, vmin=np.quantile(a, 0.90), vmax=np.quantile(a, 0.9999))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clustering = 'kmeans'\n",
    "\n",
    "if clustering == 'gmm':\n",
    "    def get_cluster_labels(weights, components=20):\n",
    "    #     weights = weights.isel(z=slice(None, None, 10))\n",
    "    #     pca = PCA(n_components=8)\n",
    "    #     weights = pca.fit_transform(weights) \n",
    "        weights_small = weights.isel(z=slice(None, None, 10))\n",
    "        mix = GaussianMixture(n_components=components)\n",
    "        mix.fit(weights_small)\n",
    "        return mix, mix.predict(weights)\n",
    "\n",
    "elif clustering == 'kmeans':\n",
    "    def get_cluster_labels(weights, components=20):\n",
    "#         weights_small = weights.isel(z=slice(None, None, 10))\n",
    "        cluster = KMeans(n_clusters=components)\n",
    "        cluster.fit(weights)\n",
    "        return cluster, cluster.labels_\n",
    "\n",
    "# N = 30\n",
    "# mix0, labels0 = get_cluster_labels(ds_diff['weights0'], N)\n",
    "# mix1, labels1 = get_cluster_labels(ds_diff['weights1'], N)\n",
    "\n",
    "ds_diff['labels0'] = ('z', ), labels0\n",
    "ds_diff['labels1'] = ('z', ), labels1\n",
    "\n",
    "X = np.stack((ds_diff['labels0'], ds_diff['labels1']), axis=-1) \n",
    "le = preprocessing.LabelEncoder()\n",
    "ds_diff['combined'] =  ('z', ),  np.array(le.fit_transform([str(x[0]) + str(x[1]) for x in X]))\n",
    "\n",
    "if clustering == 'gmm':\n",
    "    m = np.sqrt((mix1.means_**2).sum(axis=1))\n",
    "    cov = mix1.covariances_\n",
    "    s = np.array([np.sqrt(np.trace(cov[i])/N) for i in range(0, N)])\n",
    "elif clustering == 'kmeans':\n",
    "    m = np.sqrt((mix1.cluster_centers_**2).sum(axis=1))\n",
    "\n",
    "    \n",
    "ds_diff['filter'] =  ('z', ), np.logical_or.reduce([(ds_diff['labels1'] == i[0]) for \n",
    "                                                     i in np.argwhere(m > 2.5 * np.quantile(m, 0.1))])\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "for i, v in enumerate(('combined', )): \n",
    "    labels, counts = np.unique(ds_diff[v], return_counts=True)\n",
    "    da = ds_diff[v]\n",
    "    da = da.where(ds_diff['filter'])    \n",
    "    da_img = da.values.reshape(len(ds.lon), \n",
    "                               len(ds.lat)).T\n",
    "    da_img = xr.DataArray(da_img, dims=ds_a.dims , coords=ds_a.coords)\n",
    "    da_img.rio.to_raster(f\"plots/{v}.tif\")\n",
    "    da_img.plot.imshow(ax=axs, cmap='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(m, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = np.sqrt((mix1.means_**2).sum(axis=1))\n",
    "for i in np.argwhere(m > 2):\n",
    "    ds_diff['filter'] = ('z', ),  ds_diff['labels1'] == i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist((mix1.predict_proba(ds_diff['weights1'])**2).sum(axis=1), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_diff['filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l, c = np.unique(ds_diff['labels1'], return_counts=True)\n",
    "list(zip(*sorted(zip(*(l, c)), key=lambda i: i[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "centers = np.empty(shape=(mix1.n_components, ds_diff['weights1'].values.shape[1]))\n",
    "for i in range(mix1.n_components):\n",
    "    density = scipy.stats.multivariate_normal(cov=mix1.covariances_[i], mean=mix1.means_[i]).logpdf(X)\n",
    "    centers[i, :] = X[np.argmax(density)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn import mixture\n",
    "\n",
    "n_samples = 100\n",
    "C = np.array([[0.8, -0.1], [0.2, 0.4]])\n",
    "\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "         np.random.randn(n_samples, 2) + np.array([-2, 1]), \n",
    "         np.random.randn(n_samples, 2) + np.array([1, -3])]\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=3, covariance_type='full').fit(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:, 1], s = 1)\n",
    "\n",
    "centers = np.empty(shape=(gmm.n_components, X.shape[1]))\n",
    "for i in range(gmm.n_components):\n",
    "    density = scipy.stats.multivariate_normal(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(X)\n",
    "    centers[i, :] = X[np.argmax(density)]\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cov = mix1.covariances_\n",
    "N = 20\n",
    "s = np.array([np.sqrt(np.trace(cov[i])/N) for i in range(0, N)])\n",
    "m = np.sqrt((mix1.means_**2).sum(axis=1))\n",
    "plt.scatter(m, s)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(mix1.means_.mean(axis=1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(5, 15))\n",
    "for i, v in enumerate(('labels0', 'labels1', 'combined')): \n",
    "    labels, counts = np.unique(ds_diff[v], return_counts=True)\n",
    "    if v == 'combined':\n",
    "#         args = np.argwhere(np.logical_or(counts < 1000, counts == counts.max())).ravel()\n",
    "#         for a in args:\n",
    "#             ds_diff[v] = ds_diff[v].where(ds_diff[v] != labels[a])\n",
    "        da = ds_diff[v]\n",
    "    else:\n",
    "#         da = ds_diff[v]\n",
    "        da = ds_diff[v]#.where(ds_diff[v] != labels[np.argmax(counts)])\n",
    "    da_img = da.values.reshape(len(ds.lon), \n",
    "                               len(ds.lat)).T\n",
    "    da_img = xr.DataArray(da_img, dims=ds_a.dims , coords=ds_a.coords)\n",
    "#     da_img.rio.to_raster(f\"plots/{v}.tif\")\n",
    "    da_img.plot.imshow(ax=axs[i], cmap='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_cluster_labels(ds_diff['weights0'].fillna(0).isel(z=slice(0, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da_img = xr.DataArray(da_img, dims=ds_a.dims , coords=ds_a.coords)\n",
    "da_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds['labels1'] = ('lat', 'lon'), ds_diff['labels1'].values.reshape(len(ds.lon), len(ds.lat)).T\n",
    "\n",
    "def get_mean(t, label, stat):\n",
    "    filt = ds['labels1'] == label\n",
    "    return np.array([(getattr(ds[v].isel(time=t).where(filt), stat)(skipna=True).values) for v in ds.variables if v.startswith('B')])\n",
    "\n",
    "labels = np.unique(ds['labels1'])\n",
    "# labels = labels[~np.isnan(labels)]\n",
    "print(labels)\n",
    "\n",
    "labels_sorted, m_ = list(zip(*sorted(zip(*(labels, m)), key=lambda i: i[1], reverse=True)))\n",
    "\n",
    "for l in labels_sorted:\n",
    "    plt.errorbar(range(8), get_mean(0, l, 'mean'), yerr=get_mean(0, l, 'std'))\n",
    "    plt.errorbar(np.arange(8) + 0.05, get_mean(1, l, 'mean'), yerr=get_mean(1, l, 'std'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    ds_diff[f'diffs{i}'] = ('band', 'z'), components[:,:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_diff.diffs0.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rm plots/*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = np.argwhere(np.logical_and(counts > 1000, counts != counts.max())).ravel()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.array(le.fit_transform([str(x[0]) + str(x[1]) for x in X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_plots(pca_comp_weights, only_one, threshold):\n",
    "#     for c in combinations(range(8), 3):\n",
    "    \n",
    "    for c in range(8):\n",
    "        fig, ax = plt.subplots()\n",
    "        c = (c, c, c) \n",
    "        ds_comp = xr.DataArray(((pca_comp_weights[:, c]**2)).reshape(len(ds.lon), len(ds.lat), 3).T, dims=('comp','lat', 'lon'))\n",
    "        ds_comp = ds_comp.reindex(lat=ds_comp.lat[::-1])\n",
    "        vmax = np.quantile(ds_comp.values.ravel(), threshold)\n",
    "#         comps_str = \"\".join(str(i) for i in c)\n",
    "        np.abs(ds_comp).plot.imshow(rgb='comp', vmin=vmax/100, vmax=vmax, alpha=1, ax=ax)\n",
    "#         np.abs(ds_comp.isel(comp=0)).plot.contour(ax=ax)\n",
    "\n",
    "#         plt.title(comps_str)\n",
    "#         plt.show()\n",
    "    #     plt.savefig(f'plots/{comps_str}') \n",
    "#         plt.close()\n",
    "        plt.show()\n",
    "        if only_one:\n",
    "            break\n",
    "            \n",
    "# create_plots(pca_comp_weights0, True, .95)\n",
    "create_plots(pca_comp_weights1, False, .999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_co_weights0[200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_plots(pca_comp_weights0_, pca_comp_weights1_, only_one, threshold):\n",
    "    global alpha\n",
    "#     for c in combinations(range(8), 3):    \n",
    "    ds_comp0 = xr.DataArray(((pca_comp_weights0_[:,  (0, 1, 2)]**2)).reshape(len(ds.lon), len(ds.lat), 3).T, dims=('comp', 'lat', 'lon'))\n",
    "    ds_comp0 = ds_comp0.reindex(lat=ds_comp0.lat[::-1]) \n",
    "    for c in range(8):\n",
    "        fig, ax = plt.subplots()    \n",
    "        ds_comp1 = xr.DataArray(((pca_comp_weights1_[:, (c, c, c)]**2)).reshape(len(ds.lon), len(ds.lat), 3).T, dims=('comp', 'lat', 'lon'))\n",
    "        ds_comp1 = ds_comp1.reindex(lat=ds_comp1.lat[::-1])\n",
    "        vmax = np.quantile(ds_comp1.values.ravel(), threshold)\n",
    "\n",
    "#         comps_str = \"\".join(str(i) for i in c)\n",
    "        alpha = 1*ds_comp1.isel(comp=0)/ds_comp1.isel(comp=0).max()\n",
    "        (alpha.expand_dims(band=3)*cube_rgb).plot.imshow(rgb='band', \n",
    "                             alpha=alpha, \n",
    "#                              vmin=vmax/100, vmax=vmax, ax=ax\n",
    "                                    )\n",
    "#         np.abs(ds_comp.isel(comp=0)).plot.contour(ax=ax)\n",
    "\n",
    "#         plt.title(comps_str)\n",
    "#         plt.show()\n",
    "    #     plt.savefig(f'plots/{comps_str}') \n",
    "#         plt.close()\n",
    "        plt.show()\n",
    "        if only_one:\n",
    "            break\n",
    "       \n",
    "            \n",
    "# create_plots(pca_comp_weights0, True, .95)\n",
    "create_plots(pca_comp_weights0, pca_comp_weights1, False, .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube_rgb = cube_rgb.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube_rgb.where(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "da = xr.DataArray(components, dims=('band', 'z', 'comp'))\n",
    "da_z = da.stack(zz=('comp', 'band'))#.transpose('zz', 'band')\n",
    "da_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "kmeans = KMeans(n_clusters=30, random_state=0).fit(da_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    x = x/np.sqrt((x**2).sum())\n",
    "    x -= x.min()\n",
    "    return x\n",
    "\n",
    "for c in kmeans.cluster_centers_:\n",
    "    plt.plot(norm(c[:8]))\n",
    "    plt.plot(norm(c[8:]))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_z = da_z.to_dataset(name='weights')\n",
    "ds_z['labels'] = kmeans.labels_\n",
    "plt.imshow(ds_z.unstack()['labels'].values.reshape(len(ds.lon), len(ds.lat)).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = (np.abs(ds_comp)).max(dim='comp')\n",
    "# vmax = np.quantile(x.values.ravel(), .99)\n",
    "vmin = np.quantile(x.values.ravel(), .98)\n",
    "(x > vmin).plot.imshow(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.components_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_comp2d(c):\n",
    "    return xr.DataArray(components[:,:,c].prod(axis=0).reshape(len(ds.lon), len(ds.lat)).T, dims=('lon', 'lat'))\n",
    "\n",
    "for i in range(2):\n",
    "    d = np.abs(get_comp2d(i))\n",
    "    d = d / d.sum()\n",
    "    plt.imshow(d, vmin=0, vmax=np.quantile(d.values.ravel(), .98))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_rgb = xr.concat([get_comp2d(1), get_comp2d(0), get_comp2d(0)], 'band')\n",
    "d_rgb.plot.imshow(rgb='band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = get_comp2d(0)*(10**9)*get_comp2d(1)\n",
    "plt.imshow(d, vmax=np.quantile(d.ravel(), .99))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(d.ravel(), bins=1000)\n",
    "# plt.xlim(-0.01, 0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = np.abs(pca.components_[1].reshape(len(ds.lon), len(ds.lat)))**1\n",
    "plt.imshow(d.T, vmin=0, vmax=np.quantile(d,.99))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da_z['components'] = (() pca.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube_z.isel(time=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set Sentinel Hub credentials\n",
    "# sh_credentials = dict(client_id=environ['SH_CLIENT_ID'],\n",
    "#                       client_secret=environ['SH_CLIENT_SECRET'])\n",
    "# sh_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PlanetScope (directly from Sentinel Hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CLIENT_ID = environ['SH_CLIENT_ID']\n",
    "# CLIENT_SECRET = environ['SH_CLIENT_SECRET']\n",
    "\n",
    "# PlanetScope\n",
    "# INSTANCE_ID = '' # instance_id  corresponding to PlanetScope instance (can be found at Sentinel-hub dashboard)\n",
    "COLECTION_ID = '' # colleciton_id (can be found at Sentinel-hub dashboard at instance configuration)\n",
    "LAYER_ID = '1_TRUE-COLOR'  # layer_id can be found at Sentinel-hub dashboard at instance configuration\n",
    "\n",
    "# config = SHConfig()\n",
    "# config.sh_client_id = CLIENT_ID\n",
    "# config.sh_client_secret = CLIENT_SECRET\n",
    "# config.instance_id = INSTANCE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_areas = pd.read_csv('data/Areas_of_interest - VHR data.csv', skiprows=2)\n",
    "\n",
    "def get_bbox(lat, lon):\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3857\")\n",
    "    x2, y2 = transformer.transform(lat, lon)\n",
    "    print(x2, y2)\n",
    "\n",
    "    def coor(axis,res,point):\n",
    "        return res*axis\n",
    "    res = partial(coor, 500, 3)\n",
    "\n",
    "    offsetx, offsety = 1*res(x2), 1*res(y2)\n",
    "    coords = [x2 - offsetx, y2 - offsety, x2 + offsetx,  y2 + offsety]\n",
    "    return BBox(bbox=coords, crs=3857)\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Iterate through all the locations and yield a dataarray containg the time series\n",
    "    of images\n",
    "    \"\"\"\n",
    "    for country, location, lat, lon in zip(df_areas['Country'], \n",
    "                                  df_areas['Location'], \n",
    "                                  df_areas['Lat'], \n",
    "                                  df_areas['Lon']):\n",
    "        try:\n",
    "            if '' in location:\n",
    "                bbox = get_bbox(lat, lon)\n",
    "                byoc_request = WmsRequest(\n",
    "                    data_source=DataSource(COLECTION_ID),\n",
    "                    layer=LAYER_ID,\n",
    "                    bbox=bbox,\n",
    "                    width=500,\n",
    "                    time=('2019-01-01', '2020-06-01'),\n",
    "                    config=config)\n",
    "                data = byoc_request.get_data()\n",
    "                if data:\n",
    "                    da = xr.DataArray(data, dims=('time', 'x', 'y', 'band'), name='bands')\n",
    "                    da = da.assign_coords(time=byoc_request.get_dates())\n",
    "                    yield f'{country}, {location}', da\n",
    "        except Exception as excep:\n",
    "            print(excep)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recon(weights, component):\n",
    "    \"Reconstruct the image from the weights a components\"\n",
    "    return (weights * component).sum(dim='comp')\n",
    "\n",
    "# dsz = ds[band].stack(z=('x', 'y')).to_dataset()\n",
    "# comps = pca1.fit_transform(dsz[band].where(np.isfinite(dsz[band]), 0).T)\n",
    "# dsz['comps'] = xr.DataArray(comps, dims=('z', 'comp'))\n",
    "# df = pd.DataFrame(pca1.components_.T).set_index(dsz.time.values)\n",
    "\n",
    "def spatial_pca(da, n_components):\n",
    "    global comps\n",
    "    \"\"\"\n",
    "    Do the PCA on the time series of images\n",
    "    \"\"\"\n",
    "    pca1 = PCA(n_components=n_components)\n",
    "    dsz = da.stack(z=('x', 'y'))#.to_dataset()\n",
    "    comps = pca1.fit_transform(dsz.fillna(0).T)\n",
    "    comps = xr.DataArray(comps, dims=('z', 'comp'))\n",
    "    df = pd.DataFrame(pca1.components_.T).set_index(dsz.time.values)\n",
    "    return df, dsz\n",
    "\n",
    "def pca_pca(weights1, weights2): \n",
    "    \"Compute the PCA\"\n",
    "    global weight12\n",
    "    weight12 = pd.concat([weights1, weights2], axis=1)\n",
    "    pca = PCA()\n",
    "    return pca.fit_transform(weight12)\n",
    "    \n",
    "def norm_weights(w):\n",
    "    #scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "    #w = scaler.fit_transform(w.values.reshape(-1, 1))\n",
    "    #return pd.Series(w.ravel())\n",
    "    return w    \n",
    "\n",
    "def pca_pca_recon(df, dsz, start1, stop1, start2, stop2):\n",
    "    \"\"\"\n",
    "    Do a PCA on the meaned PCA weights to generate a change image\n",
    "    \"\"\"\n",
    "    global weights1_org, weights2_org, compons\n",
    "    compons = dsz.unstack().transpose('x', 'y', 'time')\n",
    "    weights1_org, weights2_org = df[start1: stop1].mean(), df[start2: stop2].mean()\n",
    "    weights1 = norm_weights(weights1_org)\n",
    "    weights2 = norm_weights(weights2_org)\n",
    "    # Do the PCA \n",
    "    for i in range(5): # Do a few iterations to remove bias\n",
    "        y = pca_pca(weights1, weights2)\n",
    "        weights1, weights2 = pd.Series(y[:,0]), pd.Series(y[:,1])\n",
    "    \n",
    "    # Reconstructions\n",
    "    \n",
    "    # Original image\n",
    "    rec_org1 = xr.DataArray(recon(weights1_org.values.reshape(1,-1), compons).unstack(), dims=('x', 'y'))\n",
    "    rec_org2 = xr.DataArray(recon(weights2_org.values.reshape(1,-1), compons).unstack(), dims=('x', 'y'))\n",
    "    \n",
    "    # PCA compenents. Reconstruct using orginal components\n",
    "    rec1 = xr.DataArray(recon(y[:,0], compons), dims=('x', 'y')) # Correlation image\n",
    "    rec2 = xr.DataArray(recon(y[:,1], compons), dims=('x', 'y')) # Change image\n",
    "    ds_ch = xr.Dataset({'rec_org1': rec_org1,\n",
    "                        'rec_org2': rec_org2,\n",
    "                        'correlation': rec1, \n",
    "                        'change': rec2,\n",
    "                        'components': compons})\n",
    "    \n",
    "    return df, ds_ch, y\n",
    "\n",
    "def change_dectection(da, cloud_mask):\n",
    "    global df\n",
    "    \"Do the change detection\"\n",
    "    n_components = min(len(da.time), 20)\n",
    "    # Fill missing values with the mean values across the time dimension\n",
    "    da = da.where(cloud_mask, da.where(np.logical_not(cloud_mask)).mean(dim=('x', 'y')))\n",
    "    # Do the spatial PCA\n",
    "    df, dsz = spatial_pca(da, n_components) \n",
    "    # Do the PCA for change detection \n",
    "    df, ds_ch, weights = pca_pca_recon(df, dsz,  '2020-11-30', '2020-12-14', '2020-12-15', '2020-12-31')\n",
    "    return ds_ch, df, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "band = 'B03'\n",
    "n_components = min(len(cube[band].time), 20)\n",
    "n_components = None\n",
    "pca1 = PCA(n_components=n_components)\n",
    "cubez = cube[band].stack(z=('x', 'y'))\n",
    "weights = pca1.fit_transform(cubez.fillna(0))\n",
    "weights = xr.DataArray(weights, dims=('z', 'comp'))\n",
    "# df = pd.DataFrame(pca1.components_.T).set_index(dsz.time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cube[band].isel(time=1).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(pca1.components_[0].reshape(cube[band][0].shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(pca1.components_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cubez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cubez.fillna(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dsz.unstack().isel(time=20).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compons.isel(time=5).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "da = cube['B03']\n",
    "cloud_mask = cube.CLM\n",
    "\n",
    "start1, stop1, start2, stop2 = '2020-11-30', '2020-12-14', '2020-12-15', '2020-12-31'\n",
    "\n",
    "n_components = min(len(da.time), 20)\n",
    "# Fill missing values with the mean values across the time dimension\n",
    "da = da.where(cloud_mask, da.where(np.logical_not(cloud_mask)).mean(dim=('x', 'y')))\n",
    "# Do the spatial PCA\n",
    "df, dsz = spatial_pca(da, n_components) \n",
    "\n",
    "compons = dsz.unstack().transpose('x', 'y', 'time')\n",
    "weights1_org, weights2_org = df[start1: stop1].mean(), df[start2: stop2].mean()\n",
    "weights1 = norm_weights(weights1_org)\n",
    "weights2 = norm_weights(weights2_org)\n",
    "# Do the PCA \n",
    "for i in range(5): # Do a few iterations to remove bias\n",
    "    y = pca_pca(weights1, weights2)\n",
    "    weights1, weights2 = pd.Series(y[:,0]), pd.Series(y[:,1]) \n",
    "\n",
    "# Reconstructions\n",
    "\n",
    "# Original image\n",
    "rec_org1 = xr.DataArray(recon(weights1_org.values.reshape(1,-1), compons).unstack(), dims=('x', 'y'))\n",
    "rec_org2 = xr.DataArray(recon(weights2_org.values.reshape(1,-1), compons).unstack(), dims=('x', 'y'))\n",
    "\n",
    "# PCA compenents. Reconstruct using orginal components\n",
    "rec1 = xr.DataArray(recon(y[:,0], compons), dims=('x', 'y')) # Correlation image\n",
    "rec2 = xr.DataArray(recon(y[:,1], compons), dims=('x', 'y')) # Change image\n",
    "ds_ch = xr.Dataset({'rec_org1': rec_org1,\n",
    "                    'rec_org2': rec_org2,\n",
    "                    'correlation': rec1, \n",
    "                    'change': rec2,\n",
    "                    'components': compons})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "change_dectection(cube['B03'], cube.CLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weight12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "changes, df, ws = zip(*[change_dectection(cube[b], cube.CLM) for b in ['B03', 'B04', 'B08']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collect_bands(arr, variable):\n",
    "    return xr.concat([arr[b][variable].assign_coords(band=b).expand_dims('band') for b in range(3)], \n",
    "                     dim='band')\n",
    "\n",
    "def norm(X, low=0.01, high=0.99):\n",
    "    Xmin = X.quantile(low, dim=('x', 'y'))\n",
    "    Xmax = X.quantile(high, dim=('x', 'y'))\n",
    "    return (X - Xmin) / (Xmax - Xmin)\n",
    "\n",
    "#da = da.drop_sel(time=da.time.isel(time=1).values) # temporary\n",
    "\n",
    "changes, df, ws = zip(*[change_dectection(cube[b], cube.CLM) for b in ['B03', 'B04', 'B08']])\n",
    "corr = collect_bands(changes, 'correlation')\n",
    "chan = collect_bands(changes, 'change')\n",
    "rec_org1 = collect_bands(changes, 'rec_org1')\n",
    "rec_org2 = collect_bands(changes, 'rec_org2')\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 15/3))\n",
    "fig.suptitle(location)\n",
    "fig.text(0.6, 0,'Contains modified PlanetScope data processed by Euro Data Cube')\n",
    "not_bright = (norm(np.abs(corr).sum(dim='band')) < 0.99)\n",
    "change = np.clip(norm((chan.sum(dim='band')**2))**2, 0, 1).where(not_bright).fillna(0)\n",
    "change.drop('quantile').plot.imshow(ax=axs[0], add_colorbar=False)\n",
    "axs[0].set_title('Change')\n",
    "\n",
    "ch1 = collect_bands(changes, 'rec_org1')\n",
    "np.clip(norm(ch1, 0, 1), 0, 1).drop('quantile').plot.imshow(rgb='band', ax=axs[1])\n",
    "axs[1].set_title('March - May 2019')\n",
    "\n",
    "ch2 = collect_bands(changes, 'rec_org2')\n",
    "np.clip(norm(ch2, 0, 1), 0, 1).drop('quantile').plot.imshow(rgb='band', ax=axs[2])\n",
    "axs[2].set_title('March - May 2020')\n",
    "\n",
    "plt.savefig('plots/' + location.replace(' ', '').replace(',', '').replace('/', ''), dpi=300)\n",
    "#plt.savefig('plots/' + \"{:03d}\".format(i), dpi=300)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[(v, ds[v].mean().values) for v in ds.isel(time=0).variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "properties": {
   "description": "Access data sets through the EDC Sentinel Hub API via xcube",
   "id": "523c08c6-46a4-4be3-a2f2-57f8139a2048",
   "license": "MIT",
   "name": "Using xcube to access non-commercial and commercial data sets",
   "requirements": [
    "eurodatacube"
   ],
   "tags": [
    "EO Data",
    "Sentinel Hub",
    "xcube"
   ],
   "tosAgree": true,
   "type": "Jupyter Notebook",
   "user_id": "fd3df931-a78a-4ba9-b4d4-4da1606125fd",
   "version": "0.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
